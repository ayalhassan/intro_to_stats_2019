---
title: "Lab9 - Ordered and Multinomial Logistic Regression"
author: "Barum Park"
date: "11/22/2019"
output: 
    html_document:
        keep_md: false
        matjax: default
        theme: yeti
        highlight: textmate
        toc: true
---

<style type="text/css">

body{ 

    font-size: 16px;
    line-height: 1.7em;

}

blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 16px;
    border: solid 1px;
}

h1 { font-size: 32px; }

h2 { font-size: 24px; }

h3 { font-size: 20px; }

.nobullet li {
  list-style-type: none;
}

</style>

<br>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = FALSE,
                      fig.align = "center",
                      fig.width = 4.5,
                      fig.height = 4,
                      letina = TRUE)
```


To understand **ordinal and multinomial logistic regression**, the best point to start is the **latent variable formulation** of the (binary) logistic regression model. The latent variable representation often appears in econometric textbooks, especially in the derivation of the probit model. We will first discuss the probit model, then show that the same model with a different distributional assumption of th error term leads to the logit model, and then pivot to ordinal and multinomial logit models. 

<br>

# Latent Variable Formulation of the Probit Model

Suppose you have a "latent" (i.e., unobserved) outcome $y^\ast$ which is continuous. We assume that this latent variable is generated by the following data-generating process
$$Y^\ast = \alpha + X_i\beta + \epsilon^\ast.$$
Suppose further that the error term, which we don't observe as well, has the following distribution
$$\epsilon^\ast \sim \text{Normal}(0,1),$$ 

If we would observe the latent variable $Y^\ast$, we could simply run an OLS regression to estimate the regression coefficients $(\alpha, \beta)$. Unfortunately, by construction, $Y^\ast$ is unobservable (it's latent!). Suppose that we observe only a truncated version of the latent variable. Namely, we observe the variable $Y$ which is related to the latent variable $Y^\ast$ by the following relationship:
$$Y = \begin{cases}1, \quad &\text{if } Y^\ast > 0  \\ 
 	0, &\text{otherwise} \end{cases}$$
You can think of the value $0$ as a "threshold" that determines what outcome we observe in the data: if $Y^\ast$ is larger than 0, we observe $Y = 1$; if $Y^\ast$ is smaller than zero, we observe $Y=0$. 

One point to notice, which will help us later, is that this threshold is arbitrary. For example, we could choose any other value as the threshold and obtain the same outcome by re-defining the constant term of the latent regression equation. To illustrate, let us choose an arbitrary value $a$ as the new threshold; then we would observe $Y = 1$ only if $Y^\ast > a$. Further, the probability of observing $Y = 1$ would be
$$\begin{aligned}
	 \Pr[Y^\ast > a] &= \Pr[\alpha + \beta X + \epsilon^\ast > a] \\
	 &=\Pr[ (\alpha- a) + \beta X + \epsilon^\ast > 0] \\
	 &=\Pr[ \alpha^\ast + \beta X + \epsilon^\ast > 0],
	 \end{aligned}$$
where $\alpha^\ast = \alpha - a$. Hence, even if our initial threshold value was $a$, we can simply redefine the constant term of the regression as $\alpha^\ast = \alpha - a$, and change the threshold value back to zero. For both of these models, the probability of observing $Y = 1$ would be exactly the same. In fact, we could also have decided that we don't want any constant in our regression equation at all and set the threshold to $-\alpha$; all of these different formulations would lead to the same probabilities of $Y = 1$.

The model with the latent variable $Y^\ast$ and the observed binary variable $Y$ is called the **probit** model. As with other regression models, our goal is to estimate the regression coefficients $\alpha$ and $\beta$. To do so, we observe that the following holds: Let $\Phi$ be the cumulative distribution function of a standard Normal random variable. That is, $\Phi(x) = \Pr[X \le x]$, where $X \sim \text{Normal}(0,1)$. Recall that the probit model assumes that the latent error term $\epsilon^\ast$ follows this distribution, i.e., $\epsilon^\ast \sim \text{Normal}(0,1)$. Then, we have

$$\begin{aligned}
\pi_i(x_i) &= \Pr[Y_i = 1\,\vert\,X_i = x_i] & \\
&= \Pr[Y_i^\ast > 0\,\vert\, X_i = x_i] \\
&= \Pr[\alpha + \beta x_i + \epsilon_i^\ast > 0] \\
&= \Pr[\epsilon_i^\ast >  - (\alpha + \beta x_i)] \\
&= 1-\Pr[\epsilon_i^\ast \le - (\alpha + \beta x_i)] \\
&= 1 - \Phi\left(-[\alpha + \beta x_i]\right) \\
&= \Phi\left(\alpha + \beta x_i\right),
\end{aligned}$$
where the last step follows from the fact that the standard Normal distribution is symmetric about zero. A regression model of the form

$$\pi_i(x_i) = \Phi\left(\alpha + \beta x_i\right)$$

is called a probit model. Notice that the left-hand side is the probability that a Bernoulli random variable, $Y_i$, is equal to one (conditional on the predictor(s) $X_i$). The right-hand side is the combination of a link-function $\Phi$ and a linear combination of our predictor(s). Thus, everything is the same as the logistic regression model, except for the link function.

<br>

# Latent Variable Formulation of the Logit Model

The logistic regression model can be derived in the same way, by changing the distributional assumption of the "latent" error term, $\epsilon^\ast$. Namely, for the logistic regression model,  we assume that $\epsilon^\ast \sim \text{Logistic}(0, 1)$, i.e., that the error term follows a Logistic distribution with location and scale parameters, respectively, equal to zero and one. Again, I hope you are not scared away by the appearance of a new distribution. The Logistic distribution looks quite similar to the Normal distribution; you can think of it as "just" another distribution that people have invented in the past.

To give you an intuitive feeling of the $\text{Logistic}(0,1)$ distribution, we might plot it together with a $\text{Normal}(\mu = 0, \sigma = 1.8)$ distribution:

```{r, fig.height = 4.5}
# values at which to plot distributions
x = seq(-5, 5, .05)

# plot Normal(0, 1.8) distribution first
plot(x, dnorm(x, 0, 1.8), type = "l", col = "darkcyan",
     ylab = "Density", xlab = NA, ylim = c(0, .3))
# add Logistic(0,1) distribution
lines(x, dlogis(x), col = "darkmagenta")

# add legend
legend("topright", col = c("darkcyan", "darkmagenta"),
       lty = c(1,1), legend = c("Normal (0, 1.8)", "Logistic(0, 1)"),
       bty = "n")
```

As you can see the distributions look very similar; both distributions are symmetric around its mean and are bell-shaped. 

The cumulative distribution function of a logistic random variable, call it $\Lambda$, has a now-familiar form:

$$\Lambda(x) = \frac{1}{1 + e^{-x}}.$$

Notice that this is exactly the the same function we have denoted by $\text{logit}^{-1}$ in the [previous lab](https://htmlpreview.github.io/?https://github.com/baruuum/intro_to_stats_2019/blob/master/Lab8/Lab8.html#treating-pid-as-a-categorical-variable). 

Now, notice that the first five equations in the derivation of the probit model will hold regardless of which distribution we assume for $\epsilon_i^\ast$ (we are using only the basic properties of probability); the sixth line, on the other hand, will us lead us to $1 - \Lambda(-[\alpha + \beta x_i])$, but as the logistic distribution is symmetric, similarly to the Normal distribution, we arrive at 

$$\pi_i(x_i) = \Lambda(\alpha + \beta x_i) = \text{logit}^{-1}(\alpha + \beta x_i),$$

which is exactly the same formulation of the logistic regression model that was introduced in the [previous lab](https://htmlpreview.github.io/?https://github.com/baruuum/intro_to_stats_2019/blob/master/Lab8/Lab8.html#treating-pid-as-a-categorical-variable). In other words, the data-generating process of the logistic regression model can be expressed as "latent" linear regression where the error term has a $\text{Logistic}(0,1)$ distribution.

Let me convince you that these are the same models by simulation. Namely, let us simulate a sample from the latent variable formulation, fit a logistic regression model, and compare the true coefficients to the estimated coefficients. If these two sets of coefficients are sufficiently close, we might be convinced that the latent variable representation and the logit representation we dealt with in the previous lab might be indeed the same models:

```{r}
# set seed
set.seed(123)

# set observations
n = 25000

# beta-vector (constant, beta1, beta2)
beta = c(-.5, .75, .2)

# first predictor (continuous)
x1 = runif(n, -1, 1)

# second predictor (dummy)
x2 = sample(c(0,1), n, replace = T)

# generate latent error term ~ Logistic(0, 1)
e_star = rlogis(n)

# generate latent outcome y_star
y_star = beta[1] + beta[2] * x1 + beta[3] * x2 + e_star

# set observed outcome equal to 1 if y_star > 0 and set it to 0 otherwise
y = ifelse(y_star > 0, 1, 0)

# fit a logistic regression to the simulated dataset
lreg = glm(y ~ x1 + x2, family = binomial(link = "logit"))

# compare true to estimated coefficients
rbind(Estimated = coef(lreg),
      True = beta)
```
which is quite close! 

<br>

# Ordered Logistic Regression

You might ask why we go through all of this. The reason is that the latent variable formulation can be easily extended to the **ordered logistic regression model**. Recall that we had one threshold in the latent variable formulation of the probit/logit model, namely the value $0$. For the ordered logistic regression model, we consider multiple thresholds.

Let us start with the model
$$Y_i^\ast = \alpha + \beta_i X_i + \epsilon_i^\ast,$$
where $\epsilon_i^\ast \sim \text{Logistic}(0,1)$. But for the ordinal logistic regression case, we change two things. 

1. Instead of estimating a constant of the regression and setting the threshold to zero, we set the constant to zero and estimate the threshold. As shown above, this will lead to the same probabilities as
$$\alpha + \beta x_i + \epsilon_i^\ast> 0 \iff \beta x_i + \epsilon_i^\ast > - \alpha.$$
Hence, by defining $\tau_1 = -\alpha$ as our first threshold and assuming that the latent regression model has no constant, we will obtain the same probabilities.

2. To link the latent regression with the observed outcome, which is no longer binary but has $K$ ordered responses, we will to estimate $K - 1$ threshold parameters, namely $\tau_1, \tau_2, ..., \tau_{K-1}$. 

This is all fairly abstract. So, let us use the simplest example: an ordered logistic regression where the outcome has 3 ordered categories---i.e., $Y_i \in \{1,2,3\}$---and we have a single predictor, $X_i$. To link the latent variable to the observed one, we will need two thresholds, which we denote by $\tau_1$ and $\tau_2$. The relationship between the latent and observed outcome is then modeled by the function:
$$Y_i = \begin{cases}
	1, \quad &\text{if } Y_i^\ast < \tau_1^\ast \\
	2, \quad &\text{if } \tau_1^\ast \le Y_i^\ast < \tau_2^\ast \\
	3, &\text{if } Y_i^\ast \ge \tau_2
	\end{cases}$$
Suppose $\tau_1 = -0.5$ and $\tau_2 = 2$, then we might imagine the relationship between $\beta X_i$ and the response probabilities in the following way. Suppose $\beta = 0.75$ and and $X_i = 0$. Then $Y_i^\ast = \epsilon_i^\ast$, where $\epsilon_i^\ast \sim \text{Logistic}(0,1)$. So, $Y^\ast \sim \text{Logistic}(0,1)$ and the probabilities to observe the outcomes $1,2,3$ would be  
```{r}
# grid to plot distribution
x = seq(-7, 7, .01)

# threshold values
tau = c(-0.5, 2)

# plot logistic curve
plot(x, dlogis(x), type = "l", lty = 3, col = "darkmagenta",
     xlab = expression(paste(Y[i]^"*", " = ", epsilon[i]^"*")),
     ylab = "")

# plot thresholds
abline(v = tau, col = "grey")

# plot outcome labels
text(-2.75, .2, expression(paste(Y[i], " = 1")))
text(1, .2, expression(paste(Y[i], " = 2")))
text(4, .2, expression(paste(Y[i], " = 3")))
```
So, we see that the outcome $1$ is most likely, followed by $3$, and $2$. On the other hand, if $X_i = 2$, then $Y_i\ast = 1.5 + \epsilon_i^\ast$, and $Y_i^\ast \sim \text{Logistic}(1.5, 1)$. So, the plot would look something like the following:
```{r}
# plot logistic curve
plot(x, dlogis(x, 1.5), type = "l", lty = 3, col = "darkmagenta",
     xlab = expression(paste(Y[i]^"*"," = 0.75 + ", epsilon[i]^"*")),
     ylab = "")

# plot thresholds
abline(v = tau, col = "grey")

# plot outcome labels
text(-2.75, .2, expression(paste(Y[i], " = 1")))
text(1, .2, expression(paste(Y[i], " = 2")))
text(4, .2, expression(paste(Y[i], " = 3")))
```
So, now the outcome $3$ will have the highest probability of occurring, followed by $2$ and $2$. In other words, the value of $X_i$ and $\beta$ will push the center of the logistic distribution around, which will change how each of the outcomes $1, 2$, and $3$ are. When $\beta > 0$ higher outcomes will become more likely as $X_i$ increases; if $\beta < 0$ the opposite relation will hold.

Let me, again, illustrate how this work via simulation. To fit a ordered logistic regression model in R, we'll use the `polr` function from the `MASS` package. So, let us first install the package:
```{r, eval = F}
install.packages("MASS")
```
We load also some other packages that we need:
```{r, message = F}
library(dplyr)
library(data.table)
library(dtplyr)
library(ggplot2)
```
Now, here comes the simulation:
```{r}
# obs
n = 25000

# coefficients
beta = c(.8, .25)

# predictors (dummy & continuous)
x1 = sample(c(0, 1), n, replace = T)
x2 = runif(n, -2, 2)

# thresholds
tau = c(-.5, 2)

# generate latent outcome
ystar = x1 * beta[1] + x2 * beta[2] + rlogis(n)

# generate obseved outcome
y = ifelse(
    ystar < tau[1], 1,
        ifelse(ystar >= tau[1] & ystar < tau[2], 2, 3)
)

# construct dataset
olog_dat = data.table(y, x1, x2)

# fit ordered logistic regression          
olog_fit = MASS::polr(factor(y) ~ x1 + x2, data = olog_dat)

# summarize
summary(olog_fit)
```
This is, again, extremely close, which should be expected given that the sample size is large and our estimator is consistent.

The interpretation of these coefficients is a little bit annoying. For example, the estimate $\hat\beta_2 = .253$ would show us that a unit-increase in $X_2$ is associated with a $.253$ increase of the logged odds of giving a "higher" response. But what does this even mean? As with other models, plotting your results is often the most helpful way to present them.

For the simulate data, we might first fix `x1 == 1` and let `x2` vary and see how the predicted probabilities of the three categories change with changes in `x2`. 
```{r, fig.width = 5.5}
# new dataset for predictions
new_dat = data.table(x1 = 1, x2 = seq(-2, 2, .1))

# obtain predicted probabilities
yhat = data.table(
    new_dat[, x2],
    predict(olog_fit, newdata = new_dat, type = "p")
) 

# rename variables
setnames(
    yhat, 
    c("x2", "Pr[Y = 1]", "Pr[Y = 2]", "Pr[Y = 3]")
)

# reshape into long form
yhat_long = melt(
    data = yhat, 
    id.vars = "x2",
    variable.name = "outcome",
    value.name = "pred.prob")

# plot
ggplot(yhat_long, aes(x = x2, y = pred.prob, col = outcome)) + 
    geom_line() +
    theme_bw() +
    labs(y = "Predicted Probabilities",
         x = expression(x[2])) +
    scale_color_viridis_d(
        name = "Outcomes",
        begin = .2, 
        end = .8, 
        option = "A")
```
Unfortunately, there is no straightforward way to calculate uncertainty estimates for these predictions. In fact, I was very surprised to find that setting `se = TRUE` in the `predict` function on a `polr` object doesn't work.

In these situations, we can always rely on simulations to quantify our uncertainty. The material is a little bit advanced since it requires some knowledge of likelihood theory. So, it is added as an appendix.

<br>

# Multinomial Logistic Regression

The **multinomial logistic regression model** is used when the outcome is categorical but when the categories cannot be ordered. While there is also a latent variable formulation for this model, the derivation is much more technical and involves an understanding of the Extreme Type I (Gumbel) distribution. Hence, here I'll skip this step and focus on how to simulate data from these models and how to fit them in R.

Recall that the multinomial logit model assumes that the probabilities of the outcomes $Y_i = 1,2,...,K$ are given as

$$\begin{aligned}
\pi_{i}^{(1)}(x_i) & = \Pr[Y_i = 1\,\vert\, X_i = x_i] = \frac{\exp(\alpha_1 + \beta_1 x_i)}{\sum_{k=1}^K \exp(\alpha_k + \beta_k x_i)} \\
\pi_{i}^{(2)}(x_i) & = \Pr[Y_i = 2\,\vert\, X_i = x_i] = \frac{\exp(\alpha_2 + \beta_2 x_i)}{\sum_{k=1}^K \exp(\alpha_k + \beta_k x_i)} \\
&\vdots \\
\pi_{i}^{(K)}(x_i) & = \Pr[Y_i = K\,\vert\, X_i = x_i] = \frac{\exp(\alpha_K + \beta_K x_1)}{\sum_{k=1}^K \exp(\alpha_K + \beta_k x_i)} \\
\end{aligned}$$

Notice that for each $i$, the outcome probabilities have to sum to one, since each unit $i$ must "choose" any of the $K$ option. For the representation above, this holds for *any* values of $\alpha_1, ..., \alpha_K, \beta_1,...,\beta_K$. That is, regardless of the values we choose for the regression coefficients, the following relationship will *always* hold:
$$
\begin{aligned}
\sum_{k=1}^K \pi_i^{(k)}(x_i) &= \sum_{k=1}^K\left(\frac{\exp(\alpha_k + \beta_k x_i)}{\sum_{k=1}^K \exp(\alpha_k + \beta_k x_i)}\right) \\
& = \frac{\sum_{k=1}^K\exp(\alpha_k + \beta_k x_i)}{\sum_{k=1}^K \exp(\alpha_k + \beta_k x_i)} \\
& = 1.
\end{aligned}
$$
If the relationship holds for any values of $\{\alpha_k, \beta_k\}_{k=1}^K$, there is no way to estimate these coefficients. Hence, we need to put some constraints on the regression coefficients. In the statistics literature, these constraints are often called *identification constraints*.

The most common constraint that is imposed in the multinomial logit model is to fix one of the outcomes as the **reference category** for which the corresponding regression coefficient will be equal to zero. Which category we choose is immaterial; so, let us use the first outcome category as the reference. When we set $\alpha_1 = \beta_1 = 0$, we have $\exp(\alpha_k + \beta_1 x_1) = 1.$ Hence, we can rewrite the outcome probabilities, with the constraint imposed, as

$$\begin{aligned}
\pi_{i}^{(1)}(x_i) & = \Pr[Y_i = 1\,\vert\, X_i = x_i] = \frac{1}{1 + \sum_{k=2}^K \exp(\alpha_k + \beta_k x_i)} \\
\pi_{i}^{(2)}(x_i) & = \Pr[Y_i = 2\,\vert\, X_i = x_i] = \frac{\exp(\alpha_2 + \beta_2 x_i)}{1 + \sum_{k=2}^K \exp(\alpha_k + \beta_k x_i)} \\
&\vdots \\
\pi_{i}^{(K)}(x_i) & = \Pr[Y_i = K\,\vert\, X_i = x_i] = \frac{\exp(\alpha_K + \beta_K x_1)}{1 + \sum_{k=2}^K \exp(\alpha_k + \beta_k x_i)} \\
\end{aligned}$$

When this parameterization is used, the exponentiated regression coefficients from the multinomial logit model can be interpreted in terms of **relative risk ratio**s. This makes the interpretation of the coefficients a bit more intuitive, compared to the coefficients from either binary or ordinal logistic regression. 

The interpretation in terms of relative risk comes from the following relationship. First, the relative risk of falling into outcome category $k$ when $X_i = x_i$, compared to the risk of falling into the baseline category, is 

$$\begin{aligned}
\frac{\pi_i(x_i)^{(k)}}{\pi_i(x_i)^{(1)}} &= \frac{\frac{\exp(\alpha_k + \beta_kx_i)}{1 + \sum_{k=2}^K \exp(\alpha_k + \beta_k x_i)}}{\frac{1}{1 + \sum_{k=2}^K \exp(\alpha_k + \beta_k x_i)}} \\
&= \exp(\alpha_k + \beta_kx_i) \\
&= \exp(\alpha_k) \exp(\beta_kx_i).
\end{aligned}$$
(Notice the left-hand side is the ratio of two *probabilities* and not *odds*). Similarly, the relative risk for the same outcome $k$ for $x_i + 1$ is
$$
\frac{\pi_i(x_i + 1)^{(k)}}{\pi_i(x_i+1)^{(1)}} = \exp(\alpha_k) \exp[\beta_k(x_i + 1)].$$

Dividing the second by the first equation, we obtain 
$$\begin{aligned}
\text{Relative Risk Ratio} &= \frac{\frac{\pi_i(x_i + 1)^{(k)}}{\pi_i(x_i+1)^{(1)}}}{\frac{\pi_i(x_i)^{(k)}}{\pi_i(x_i)^{(1)}}} \\
&= \frac{\exp(\alpha_k)\exp[\beta_k (x_i + 1)]}{\exp(\alpha_k)\exp(\beta_kx_i)} \\
&=\exp(\beta_k)
\end{aligned}$$

Hence, the relative risk of falling into the category $k$, instead of the baseline category, increases by **a factor** of $\exp(\beta_k)$. 

To simulate a dataset from the multinomial logit model, we might use the following code. The code is rather complicated and you won't need to understand it for the current course. 

```{r}
# number of outcome categories
K = 4

# predictor matrix
X = cbind(1, x1, x2)

# regression coefficients (we need one element for each outcome category)
beta = cbind(
    rep(0, 3),
    c(-.75, 1.2, .4),
    c(.5, .25, -.95),
    c(.5, .4, .6)
)

# unnormalized predicted probs
raw_pred = exp(X %*% beta)

# normalize
pred = sweep(raw_pred, 1, rowSums(raw_pred), "/")

# draw outcome
y = apply(pred, 1, function(w) {
    sample.int(K, size = 1, prob = w)
})

# generate dataset
dat = data.table(y, x1, x2)
```
The important thing is that we have generated an outcome with `K = 4` categories and that the regression coefficients use to simulate the data are the following:
```{r}
beta
```
where each column represents an outcome category (hence there are `K = 4`) and each row the regression coefficients of each predictor (including the constant). Notice that the first column of the `beta` matrix is filled with zero.

Fitting multinomial logit models in R can be done with the `multinom` function of the `nnet` package. The code is quite straightforward:

```{r}
# fit multinomial logit model
mlog_fit = nnet::multinom(y ~ x1 + x2, data = dat)

# look at summary
summary(mlog_fit)
```
The estimated coefficients do not include the coefficients that were fixed to zero. Also, compared to the `beta` object that was used to simulate the data, the coefficients are returned as a matrix where the row correspond to the outcomes and the columns to the predictors. We might compare the estimated coefficients to the "true" ones:
```{r}
t(beta[, -1])
```
We, again, see that they are quite close.

To obtain the relative risk ratios, we first extract the coefficients and then exponentiated them:
```{r}
# extract reg. coef and exponentiate
risk_ratio = exp(coef(mlog_fit))

# print rel. risk ratios
risk_ratio
```

The interpretation for the variable `x1` (which is a dummy variable) would be the following. Holding other variables in the model constant,

1. units with `x1 == 1` had 3.25 times the *relative risk* of falling into category `y = 2` rather than category `y = 1` (i.e., the baseline category) compared to units with `x1 == 0`; 
2. the *relative risk* of falling into category `y = 3` compared to `y = 1` was about 28.9 percentage higher for those units with `x1 == 1`
3. the *relative probability* of falling into category `y = 4` compared to `y = 1` was about 44.4 percentage higher for those units with `x1 == 1` than those with `x1 == 0`.

I've intentionally changed the words slightly in the interpretation. The important point to notice is that the interpretation is in terms of *risk* or *probabilities* and that it is *always relative*, i.e., we compare the probability of an outcome with the probability of the baseline. 

For the continuous variable `x2`, the interpretation would go like the following: for every unit increase in `x2`, the *relative risk* of falling into category `y = 2` instead of `y = 1` increases by a *factor* of 1.54 (or increases by 54 percent).

Also, similarly to the other logit models we have encountered so far, if $\beta_k > 0$, then $\exp(\beta_k) > 1$, and thus the relative risk will increase when the corresponding independent variable increases in its value; on the other hand, $\beta_k < 0$ implies that $\exp(\beta_k) < 1$, and thus the relative risk will decrease.

The R code to calculate predicted probabilities from the model is almost exactly the same as in the ordered logistic case.

```{r, fig.width = 5.5}
# new dataset for predictions (this time fixing x1 = 0)
new_dat = data.table(x1 = 1, x2 = seq(-2, 2, .1))

# obtain predicted probabilities
yhat = data.table(
    new_dat[, x2],
    predict(mlog_fit, newdata = new_dat, type = "p")
) 

# rename variables
setnames(
    yhat, 
    c("x2", "Pr[Y = 1]", "Pr[Y = 2]", "Pr[Y = 3]", "Pr[Y = 4]")
)

# reshape into long form
yhat_long = melt(
    data = yhat, 
    id.vars = "x2",
    variable.name = "outcome",
    value.name = "pred.prob")

# plot
ggplot(yhat_long, aes(x = x2, y = pred.prob, col = outcome)) + 
    geom_line() +
    theme_bw() +
    labs(y = "Predicted Probabilities",
         x = expression(x[2]))
```
The plot shows that for lower values of `x2` the outcome `y = 3` has the highest predicted probabilities, while for higher values of `x2` the outcome `y = 4` has higher predicted probabilities. For no values of `x2` (at least when `x1` is fixed at one) are the outcomes `2` and `4` most likely to occur than `3` or `1`. 

>**EXERCISE** Reproduce the plot of predicted probabilities from the multinomial logit model, but this time fix `x1` at the value `0`. How do the predicted probabilities change?

<br>

# Appendix: Simulation-based 95% Confidence Intervals for Predicted Probabilities from the Ordered Logistic Regression Model

As always, the material in the appendix is not required for the course and only provided for the curious. In this appendix, we tackle the problem of calculating confidence intervals for the predicted probabilities from the ordered logistic regression model. 

To layout the procedure, we assume the usual regularity conditions and that that the model is correctly specified. Under these conditions, 

$$ \sqrt{n}(\hat{\boldsymbol\theta} - \boldsymbol \theta) \longrightarrow N(\mathbf 0, \mathcal I^{-1}(\boldsymbol\theta))$$

in distribution as $n \rightarrow \infty$ and where $\hat{\boldsymbol\theta} = [\hat{\boldsymbol\beta}, \hat{\boldsymbol\tau}]'$ is the maximum likelihood estimator of the regression coefficients and the threshold parameters, $\boldsymbol \theta = [\boldsymbol \beta, \boldsymbol\tau]'$ is a vector of the true coefficients, and $\mathcal I(\boldsymbol\theta)$ the Fischer Information matrix. In practice, we approximate the Fischer Information with the observed Fischer information, which is equal to the negative inverse of the Hessian matrix $\mathbf H$ (A small note: as the threshold parameters have to be ordered---i.e., $\tau_1 < \tau_2 < \cdots < \tau_{K-1}$---the model is often reparameterized to ensure the right ordering of these parameters. In this case, the negative inverted Hessian will not be equal to the observed Fischer information. For example, when we look into the `MASS:::vcov.polr` function, we see that a Jacobian adjustment is added to the Hessian. As we will rely on this function, rather than coding it ourselves, we will skip the discussion of these adjustments).

To obtain 95\% confidence intervals for our predictions, we might rely on the following procedure.

1. As we know (using somewhat sloppy notation) that 
$$\hat{\boldsymbol \theta} \sim \text{Normal}(\boldsymbol\theta, n^{-1}\mathcal I(\hat{\boldsymbol\theta})^{-1})$$
in large samples, we might draw $S$ simulation draws from this distribution. Call these draws $\boldsymbol\theta^{(1)}, \boldsymbol\theta^{(2)},..., \boldsymbol\theta^{(S)}$.
2. For each simulated $\boldsymbol\theta^{(s)}, s= 1,2,...,S$, we calculate the predicted probabilities at the values of $\mathbf X$ that we want. Suppose, for example, we want the interval for a specific value of predictors, which we denote by $\mathbf x^\ast$. Then, 
    - the predicted logit would be $v^{(s)} = \mathbf x^\ast\boldsymbol\beta^{(s)}$. 
    - the predicted probability of outcome $1$ at $\mathbf x^\ast$ is then 
    $$\hat\pi_1^{(s)} = \Pr[Y_i = 1\,\vert\, \mathbf X_i = \mathbf x^\ast] = \Lambda(\tau_1; v^{(s)}, 1)$$ 
    where $\Lambda(a; b, c)$ is the Logistic CDF with location parameter $b$, scale parameter $c$, evaluated at the point $a$;
    - the predicted probability of outcome $j$, $1<j < K$, is
    $$\hat\pi_j^{(s)}\Pr[Y_i = j\,\vert\, \mathbf X_i = \mathbf x^\ast] = \Lambda(\tau_{j}; v^{(s)}, 1) - \Lambda(\tau_{j-1}; v^{(s)}, 1)$$
    - and the predicted probability of the outcome $K$ is
    $$\hat\pi_K^{(s)} = \Pr[Y_i = K\,\vert\, \mathbf X_i = \mathbf x^\ast] = 1- \Lambda(\tau_{K-1}; v^{(s)}, 1).$$

3. This will give us $S$ simulated values of $\hat\pi_{k}^{(s)}, k = 1,2,...,K$. For each $k$, we can then summarize the distribution of $\hat\pi_k^{(s)}$ using the simulation draws. For example, to obtain the 95\% confidence intervals, we can calculate the $0.025$th and $0.975$th quantile of the distribution of $\hat\pi_k^{(s)}$ across the simulation draws.

This will give us a simulation based 95\% confidence intervals. Coding this procedure in R is not too difficult (although also not too easy). I'll use a different style here to document the function, which you'll probably encounter when you look into the source code of other R functions. The lines starting with `#'` before the function definition are there to tell the reader what the function does. A `@param` line adds specific documents for each of the arguments that are passed to the function and the `@return` line documents what the function will return. After theses lines, the function is defined in the usual way.

```{r}
#' Confidence Intervals for Predictions of Ordered Logistic Regression Model
#' 
#' A function to calculate simulation-based confidence intervals for predictions based on \code{MASS::polr} objects relying large-sample behavior of the MLE.
#'
#' @param m a \code{MASS::polr} object
#' @param newdata dataset that specifies the values for which predictions should be made
#' @param level the \eqn{\alpha}-level
#' @param n_sim number of simulation draws to use
#' @param return_fit if TRUE, the function returns also the point estimates; otherwise, only confidence intervals are returned
#' @param return_list if TRUE, a \code{list} object is returned; otherwise, the function returns an \code{array}
#' @return a list of length \code{K}, where \code{K} is the number of ordered categories of the dependent variable. Each element of this list is a \code{n} times \code{m} matrix, where \code{n} is the number of points at which the predictionis made and \code{m} is either 3 (if \code{return_fit = FALSE}) or 2. 

sim_ci_pred_polr = function(
    m,                  
    newdata,            
    level,              
    n_sim = 1000,       
    return_fit = TRUE,  
    return_list = TRUE) 
{
    
    # check whether required packages are installed
    if (!requireNamespace("abind"))
        stop("Install abind to use function")
    
    # transform newdata into matrix
    X = as.matrix(newdata)
    
    # get coefs
    beta = coef(m)
    n_beta = length(beta)
    
    # get thresholds and augment with inf
    tau = m$zeta
    n_cat = length(tau) + 1L

    # get variance-covariance matrix
    Sigma = vcov(m)
    
    message(
        paste0("Calculating ", 100 * level, 
               "% confidence intervals with ",
               n_sim, 
               " simulation draws ... ")
    )

    # simulate coefficients from multivariate Normal dist.
    sim_res = MASS::mvrnorm(
        n_sim,
        mu = c(beta, tau),
        Sigma = Sigma)
    
    # for each of simulated coefs, make predictions
    pred = lapply(1:nrow(sim_res), function(w) {
        
        # linear predictor
        xb = X %*% sim_res[w, 1:n_beta]
        
        # augmented threshold vector
        aug_tau = c(
            -Inf, 
            sim_res[w, (n_beta + 1):ncol(sim_res)], 
            Inf)
        
        # empty matrix to store results
        P = matrix(NA, nrow = nrow(X), ncol = n_cat)
        
        # fill with predicted probs
        for (i in 2:length(aug_tau)) {
            
            P[, i - 1L] = plogis(aug_tau[i], xb) - 
                plogis(aug_tau[i - 1L], xb)
                                
        }
        
        return(P)
        
    })
    
    # transform to array
    pred_arr = simplify2array(pred)
    
    # get lower and upper bound probs
    lo = 0.5 * (1 - level)
    hi = 1 - lo
    
    # calculate CIs
    ci = apply(pred_arr, 1:2, quantile, probs = c(lo, hi))
    
    if (return_fit) {
        
        # linear predictor
        xb = X %*% beta
        aug_tau = c(-Inf, tau, Inf)
        
        P = matrix(NA, nrow = nrow(X), ncol = n_cat)
        
        for (i in 2:length(aug_tau)) {
            
            P[, i - 1L] = plogis(aug_tau[i], xb) - 
                plogis(aug_tau[i - 1L], xb)
                                
        
        }
        
        # add fit and re-arrange
        res = aperm(
            abind::abind(P, ci, along = 1L),
            c(2, 1, 3)
        )
        
        # add dimension names
        dimnames(res) = list(
            NULL,
            c("fit", "lower", "upper"),
            paste0("outcome_", 1:n_cat)
        )

        
    } else {
        
        # re-arrange
        res = aperm(ci, c(2, 1, 3))
        
        # add dimension names
        dimnames(res) = list(
            NULL,
            c("lower", "upper"),
            paste0("outcome_", 1:n_cat)
        )
    
    }
    
    if (!return_list) 
        return(res)
    
    # transform back to list    
    res_list = lapply(
        seq.int(dim(res)[3]), function(w) res[ , , w]
    )
    
    # add names
    names(res_list) = dimnames(res)[[3]]
    
    return(res_list)
    
}
```

Using this function, we can calculate and plot the 95% CIs of our predictions:

```{r, fig.width = 5.5}
set.seed(52432)

# obs
n = 5000

# coefficients
beta = c(.8, .25)

# thresholds
tau = c(-.5, 2)

# predictors (dummy & continuous)
x1 = sample(c(0, 1), n, replace = T)
x2 = runif(n, -2, 2)

# generate latent outcome
ystar = x1 * beta[1] + x2 * beta[2] + rlogis(n)

# generate obseved outcome
y = ifelse(
    ystar < tau[1], 1,
        ifelse(ystar >= tau[1] & ystar < tau[2], 2, 3)
)

# construct dataset
olog_dat = data.table(y, x1, x2)

# refit model
refit = MASS::polr(factor(y) ~ x1 + x2, data = olog_dat,
                   Hess = T)

# dataset to make predictions
new_dat = data.table(x1 = 1, x2 = seq(-2, 2, .1))

# get predictions and CIs
ci = sim_ci_pred_polr(
    m = refit,          # model
    newdata = new_dat,  # data for which preds are made
    level = .95,        # desired confidence level
    n_sim = 1000)       # number of sims to use

# combine list to data.table
#
# note: the do.call function calls the function rbind
#       on the list ci. It will `rbind` each of the elements
#       in the ci object together, so that the first element
#       in ci (which is a matrix of predictions for the 
#       outcome Y = 1) on top of the second element (which 
#       contains the predictions for the outcome Y = 2),
#       and so on.
pred_dat = do.call(rbind, ci) %>%
    as.data.table

# add outcome column and column for predictor values
#
# note: as mentioned repeatedly throughout the course, when
#       you are not `merge`ing datasets, but assign columns
#       by your self directly, you have to be 100% sure that
#       the rows of your original dataset indeed correspond
#       to the rows of your added columns. In this case,
#       we are 100% sure, since we have created the object
#       ourselves. In the case you are uncertain, you should
#       always check the resulting object. 

pred_dat[, `:=`(
    outcome = rep(1:3, each = nrow(new_dat)),
    x2 = rep(new_dat$x2, 3)
)]


# change outcome to factor with appropriate labels
pred_dat[
    , outcome := factor(
        outcome,
        levels = 1:3,
        labels = paste0("Pr[Y = ", 1:3, "]"))
    ]
                        

# plot
ggplot(pred_dat, aes(x = x2, y = fit, col = outcome)) + 
    geom_ribbon(
        aes(ymin = lower, ymax = upper, fill = outcome), 
        col = NA, alpha = .3,
        inherit.aes = T
    ) + 
    geom_line() + 
    theme_bw() +
    labs(x = expression(x[2]), y = "Predicted Probability") + 
    scale_color_viridis_d(
        name = "Outcomes",
        begin = .2, 
        end = .8, 
        option = "A") + 
    scale_fill_viridis_d(
        name = "Outcomes",
        begin = .2, 
        end = .8, 
        option = "A")

```
We see that the predicted probabilities are precisely estimated---i.e., the confidence intervals tendt to be narrow---which is also expected as we have a sample size of `n = 5000`. A similar approach can be used for the multinomial logit model. I'll leave that as a future exercise to you.

Another question that is important to pursue whenever you create your own function to estimate something like confidence intervals is whether these intervals indeed contain the true parameter with the correct confidence level over repeated samples. This question is examined separately [somewhere else](https://barumpark.com/blog/2019/Ologit-Predict/); you might follow that link if you are curious.

