---
title: "Lab4 - Simple Linear Regression"
author: "Barum Park"
date: "10/4/2019"
output: 
    html_document:
        keep_md: false
        matjax: default
        theme: yeti
        highlight: textmate
        toc: true
---

<style type="text/css">

body{ 

    font-size: 16px;
    line-height: 1.7em;

}

blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 16px;
    border: solid 1px;
}

h1 { font-size: 32px; }

h2 { font-size: 24px; }

h3 { font-size: 20px; }

.nobullet li {
  list-style-type: none;
}

</style>

<br>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = TRUE,
                      fig.align = "center",
                      fig.width = 5,
                      fig.height = 4,
                      letina = TRUE)
```

The simplest regression model would be of the form

$$Y = \beta_0 + \epsilon$$

where $\beta_0$ is a constant and $\epsilon$ is a random variable. If we assume that $\epsilon$ has a mean of zero---i.e., $\text{E}[\epsilon]= 0$---then we can induce that

$$\text{E}[Y] = \text{E}[\beta_0 + \epsilon] = \beta_0 + \text{E}[\epsilon] = \beta_0.$$

Thus, this model is stating that the outcome variable $Y$ is generated by some random fluctuations (the variable $\epsilon$) around a mean (the constant $\beta_0$). Notice that this is exactly the same data-generating process that we have consider here. But, not many people would agree that this is a regression model, since **regression models are all about conditional means**.

<br>

# Conditional Means

The conditional mean of the variable $Y$ *given* $X = x$, denoted by $\text{E}[Y\,\vert\,X = x]$, is (surprise!) the mean of $Y$ for those values of $X$ which are equal to $x$. 

The plot below offers an heuristic example. We can think of fixing $X$ at some particular value $x$ and calculating the average value of the *vertical* scatter at this fixed value of $X = x$. If we would do this for all values that the random variable $X$ takes on, we would obtain something similar to the red line of that plot. The goal of regression analysis is, in essence, finding a good approximation to this red line. 

```{r, echo = F, message = F, warning = F, fig.width = 7, fig.height = 5}
library("data.table")

set.seed(123)

# save graphical pars
old_par = par()

# no of points
n = 500

# error variance
e_var = 0.5

# generate uniform x
x = runif(n, -3, 3)

# normal errors
e = rnorm(n, 0, e_var)

# conditional mean function
cond_mean = function(x) {x - 0.4 * x^2 + sin(x*2)}

# generate y
y = cond_mean(x) + e

# plot 
par(mar = rep(4, 4), oma = rep(0, 4))
plot(x, y, axes = F, ann = F, pch = 19,
     col = scales::alpha("blue", .4))
box()
curve(cond_mean(x), add = T, col = "red", lwd = 3)
points(cbind(0, 0),
    pch = 19, col = "black", cex = 1.25)

# vertical & horizontal line at zero
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)

# allow out of plot-region plotting
par(xpd = TRUE)

# add text
text(-3.75, 0, "E[Y|X = x]")
text(0, min(y) -.75, "X = x")

# reset pars
par(old_par)
```

There are two things to notice before we move on. The first is that $\text{E}[Y|X = x]$ will be a function of $x$ and not of $Y$. In the plot above for example, we see that the value $\text{E}[Y|X = x]$ will change for different values of $x$; but once we have decided on a particular $X = x$, the value of $\text{E}[Y|X = x]$ is completely determined: we just average up all the vertical scatter at that point. 

The second is that the analogy between *averaging the vertical scatter* at a particular value $X=x$ and the conditional mean $E[Y|X = x]$ is, strictly speaking, not perfect. This is because the points represent concrete values of $Y$ that have been *realized*, while $Y$ is a random variable that is characterized by a whole *distribution* at $X = x$. A more appropriate plot that emphasizes that $E[Y|X = x]$ is the mean of the distribution of $Y$ at $X = x$ would look like the following one, where the black lines represent the distribution of $Y$ at three selected points of $X = x$. It shows also that the conditional mean function (the red line) passes through the mean of these distributions.
```{r, echo = F, fig.height = 6, fig.width = 7}
# generate data.table with z = 0
df = data.table(x = x, y = y, z = 0)

# select three points of X = x
x_points = c(-1.5, 0, 1.5)

# generate E[Y|X = x]
y_points = cond_mean(x_points)

# function to get histogram
get_hist = function(w, e) {
    
    # get conditional mean
    cmean_y = cond_mean(w)
    
    # create grid
    grid_y = seq(-2.5, 2.5, .1) + cmean_y
    
    # get density
    dens = dnorm(grid_y, cmean_y, e)
    
    # generate dataset
    res = data.table(
        x = w,
        y = grid_y, 
        z = dens
    )
    
    return(rbind(res, data.table(x = NA, y = NA, z = NA)))
    
}

# generate density at conditional mean
dens_df = do.call(
    "rbind",
    lapply(x_points, get_hist, e = e_var)
)

# generate df for conditional mean line
line_df = copy(df[, "x"])[
    , y := cond_mean(x)
][
    , z := 0
][
    order(x)
]

# genrate df for vertical lines

vl_df = do.call(
    "rbind",
    lapply(x_points, function(w, d = dens_df) {
    
            # generate line
            vline = seq(0, d[x == w, max(z)], length.out = 50)

            res = data.table(
                x = w,
                y = cond_mean(w),
                z = vline
            )
                        
            return(
                rbind(
                    res,
                    data.table(x = NA, y = NA, z = NA)
                )
            )
        }
    )
)
    
    
# save old graph pars
old_par = par()

# reduce margins to zero
par(mar = rep(0, 4), oma = rep(0, 4))

# plot points and grid
plot3D::scatter3D(
    df$x, df$y, df$z,
    pch = 19,
    col = scales::alpha("blue", .2),
    colkey = FALSE,
    bty = "b2",
    ylim = c(-6.5, 3.5),
    zlim = c(0, 1),
    xlab = "X",
    ylab = "Y",
    zlab = "Density")

# add lines
plot3D::scatter3D(
    line_df$x, line_df$y, line_df$z,
    col = "red", 
    add = T, 
    type = "l",
    lwd = 4)

# add vertical lines
plot3D::scatter3D(
    vl_df$x, vl_df$y, vl_df$z,
    col = "red",
    add = T,
    type = "l",
    lwd = 2)

# add density
plot3D::scatter3D(
    dens_df$x, dens_df$y, dens_df$z,
    col = "black",
    add = T,
    type = "l",
    lwd = 2)
```
Imagining to draw such black lies for all values of $X$ would give us a better image of what the conditional distribution and the conditional mean is. So, let us keep this in mind while moving on.


<br>

# Simple Linear Regression

As you've probably already noticed, the plot describes a relationship between two variables: $Y$ and $X$. That we are interested in the conditional mean of $Y$ given $X = x$ might be understood as saying that we want to predict $Y$ from $X$. 

In general, the "true" conditional mean of $Y$ might be any arbitrary function of $X$. Indeed, it would be very surprising to find that the relationship between $\text{E}[Y|X]$ and $X$ would follow a simple formula. The problem is, however, that we won't be able to meaningfully interpret the relationship between the conditional mean of $Y$ and $X$ if the function becomes too complicated. We need a simple functional form for this task.

In **simple linear regression**, we **assume** that the relationship between $\text{E}[Y|X]$ and $X$ has the following form:

$$\text{E}[Y|X] = \beta_0 + \beta_1 X,$$

where $\beta_0$ and $\beta_1$ are constants. Notice that this is a very **strong assumption**. Indeed, it would be wrong for the plot above. If we would try to approximate the blue points with a straight line, we would get something like black line below:

```{r echo = F, fig.width = 7, fig.height = 5}
plot(x, y, axes = F, ann = F, pch = 19,
     col = scales::alpha("blue", .4))
box()
curve(cond_mean(x), add = T, col = "red", lwd = 3)
lines(x, predict(lm(y ~ x), type = "response"), 
      col = "black", lty = 2, lwd = 3)
```

This is indeed a very bad representation of the pattern we observe! You will learn how to model such patterns in the data later in the course. 

For now, let us assume that $\text{E}[Y|X] = \beta_0 + \beta_1$ is a good approximation to the "true" process that generates $Y$. That is, if we would randomly sample from this process, we would obtain a plot like the following:

```{r echo = F, fig.width = 7, fig.height = 5}
cond_mean_linear = function(x) {1.2 + .75 * x}
y_linear = cond_mean_linear(x) + e
plot(x, y_linear, axes = F, ann = F, pch = 19,
     col = scales::alpha("blue", .4))
box()
curve(cond_mean_linear(x), add = T, col = "red", lwd = 3)
```

Under these circumstances, we could find the mean of $Y$ at the value $X = x^\ast$ by pluging-in $x^\ast$ into the place of $X$ of the equation:

$$\text{E}[Y|X = x^\ast] = \beta_0 + \beta_1 x^\ast.$$

Further, we see immediately that $\beta_0$ is the mean of $Y$ at the point at which $X$ equals to zero, since

$$\begin{aligned}
\text{E}[Y|X = 0] &= \beta_0 + \beta_1 \times 0 \\ &= \beta_0.
\end{aligned}$$

Thus, $\beta_0$ will be the **intercept** of our regression model. Next, suppose we are interested in the difference between the conditional mean at $X = 1$ and that at $X = 0$. Again, we can calculate the difference  by simply plugging-in the appropriate values in the place of $X$:

$$\begin{aligned}
\text{E}[Y|X = 1] - \text{E}[Y|X = 0] &= (\beta_0 + \beta_1) - \beta_0 \\ &= \beta_1.\end{aligned}$$

Hence, $\beta_1$ shows how much the conditional mean of $Y$ changes when we increase $X$ from $0$ to $1$. In fact, this is only a very partial description of the meaning of $\beta_1$. To see why, consider increasing $X$ by one unit from any arbitrary value $x^\ast$. We see that
$$\begin{aligned}
\text{E}[Y|X = x^\ast + 1] - \text{E}[Y|X = x^\ast ] &= \Big[\beta_0 + \beta_1 (x^\ast + 1)\Big] - \Big[\beta_0 + \beta_1 x^\ast\Big]\\
& = \Big[\beta_0 - \beta_0\Big] + \beta_1\Big[(x^\ast + 1) - x^\ast\Big]\\ &= \beta_1\end{aligned}$$
again! In other words, regardless from where we increase $X$ by one unit, our model states that the conditional mean of $Y$ will increase by $\beta_1$ units. More generally, the changes in $\text{E}[Y|X]$ as $X$ varies will be the same regardless of from which point $X$ changes. Notice that this means that we can represent the relationship between $\text{E}[Y|X]$ and $X$ by a straight line as in the figure above. S, we might call the parameter $\beta_1$ the **slope** of the regression model. 

## But What About the Vertical Scatter?

Now, we might ask, "but what about the vertical scatter in the plot above?" Notice that the conditional mean function passes through the could of points, but that the points are the actual values of $Y$. So, to represent $Y$, we need a representation of the *vertical scatter at each value of $X$*. This scatter is represented by an error term, which is often denoted by $\epsilon$. Adding this to our model, we obtain

$$\begin{aligned}
Y &= \text{E}[Y|X] + \epsilon \\
&= \beta_0 + \beta_1 X + \epsilon.
\end{aligned}$$

In other words, we are assuming that the random variable $Y$ is generated by a conditional mean, which is a linear function of $X$, and some scatter around that mean. But if we take the conditional expectation of both sides, we observe that
$$\begin{aligned}
\text{E}[Y|X] &= \text{E}[\beta_0 + \beta_1 X + \epsilon |X] \\
&= \beta_0 + \beta_1X + \text{E}[\epsilon | X]
\end{aligned}$$
Hence, if our model, $\text{E}[Y|X] = \beta_0 + \beta_1 X$, is indeed the true model, it must be the case that $\text{E}[\epsilon | X] = 0$. 
Thus, the assumption that $\text{E}[Y|X] = \beta_0 + \beta_1 X$ can be broken down into two assumptions which you have learned in Siwei's lecture:

1. $Y = \beta_0 + \beta_1 X + \epsilon$ (linearity)
2. $\text{E}[\epsilon |X] = 0$ (mean independence)

These are the most important assumptions of the simple linear regression model. To make valid statistical inference regarding the parameters, we need, however, one additional assumption: namely, that the variance of $\epsilon$ at each value of $X$ is the same. This is often denoted by

3. $\text{Var}[\epsilon| X] = \sigma^2$

and called the homoskedasticity assumption. Intuitively, this assumption means that the vertical scatter of the $Y$ around the conditional mean is equal across all values of $X$. An example for which this assumption is violated is given below, where we see that the conditional variance of $Y$ is larger for greater values of $X$.

```{r echo = F, fig.width = 7, fig.height = 5}
e_het = rnorm(n, mean = 0, sd = exp(x/3))
cond_mean_linear = function(x) {1.2 + .75 * x}
y_linear = cond_mean_linear(x) + e_het
plot(x, y_linear, axes = F, ann = F, pch = 19,
     col = scales::alpha("blue", .4))
box()
curve(cond_mean_linear(x), add = T, col = "red", lwd = 3)
```

<br>

# Populations and Samples

So far, we have dealt with the population-level regression equation. We have assumed that the population *is* the generative model

$$Y = \beta_0 + \beta_1 X + \epsilon.$$

Notice that the variables here have no subscripts. It describes a *process* through which data are generated: namely, the random variable $Y$ is generated as a function of another random variable $X$ and some idiosyncratic error term $\epsilon$. You can think of this as a "machine" (as Siwei likes to call it) that draws a value of $X$ and a value of $\epsilon$ and uses the equation above to generate the outcome $Y$.  

Of course, we'll never observe this population (in fact, it might be perceived as a theory of how the world works) and always deal with samples from it. These sample will come in pairs: namely, for each unit of observation that is sampled, we get an outcome $Y_i$ and a predictor $X_i$; and the sample will consist of $n$ such pairs, $(Y_i, X_i)_{i=1}^n$, where $n$ denotes the sample size. 

As if these samples are *independently* sampled from the *same* generative process above, we would have that for each unit $i$ in our sample, the following relationship holds:

$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i.$$

An important point to remember is that this equation is also **not observed**. In know this might be confusing, but I believe the best way to think about this is to think of $(Y_i, X_i)_{i=1}^n$ as the placeholder of a *hypothetical* sample that *could* have been drawn from the population regression equation. Each time we draw $(Y_i, X_i)_{i=1}^n$ from the population, we will obtain different values in our *observed* sample. This is the exact same logic as that we followed when we were discussing the sampling distribution of a statistic.

Then what do we observe? We observe one *particular* case of the possible samples we could have drawn from the process if we were to follow the same procedure. Let us denote this particular sample with lower-case letters: $(y_i, x_i)_{i=1}^n$. Based on this particular *observed* sample, we try to make inference about the parameters of the population-level regression equation.

Simulating an observed sample from a known population regression equation might be helpful to make these concepts clearer. Let $\beta_0 = 0.3, \beta_1 = 0.5$, $X \sim \text{Normal}(-1, 0.85^2)$, and $\epsilon \vert X \sim \text{Normal}(0, \sigma^2)$ with $\sigma = 1.5$. So, our population regression equation would look like

$$Y = 0.5 + 0.75 * X + \epsilon,$$

where $X$ is Normally distributed with mean $-1$ and standard deviation $0.85$, and where the error term has also a Normal distribution, independent of $X$, with mean equal to zero and standard deviation $1.2$.

Notice that *we do not directly specify the distribution of $Y$*, as we are assuming it is generated as a function of $X$ and $\epsilon$. 

Let us generate **one sample** of size $n = 500$ the population equation:

```{r}
# load packages
library("data.table")
library("ggplot2")

# set seed
set.seed(10359)

# set sample size and parameters
n = 500
beta0 = 0.3
beta1 = 0.5
sigma = 1.5

# generate X
x = rnorm(n, mean = -1, sd = 0.85)

# generate error term
epsilon = rnorm(n, mean = 0, sd = sigma)

# generate conditional mean function
xb = beta0 + beta1 * x

# generate Y
y = xb + epsilon

# construct dataset observed
dat = data.table(y = y, x = x)
```

Notice that this simulation procedure is *different* from the simulations we have done so far. In the previous labs, we had a *concrete* population, which we called `pop`, and we drew *concrete* observations from it. Here we don't have such a dataset from which we are drawing samples. Rather, we are directly drawing samples from distributions and an equation that represents our theory about how the data comes to existence.

The object that we would observe in any real-life data analysis is `dat`. Yet, if we would run this code a second time, we will get another sample, call it `dat2`, which will have the *same* dimensions as `dat` and comes from the *same* data-generating process, but which would be *different* from the concrete dataset `dat` that we have just simulated. 

<br>

# Fitting Simple Linear Regression Models in R

Whenever you are interested in the relationship between two continuous variables, the best first step is often to just plot these variables in a scatter plot in order to get a sense of how the relationship looks like. This is also helpful to detect any severe non-linearities in the relationship, which would invalidate our "linear" regression assumption. Of course, as we have generated the data ourselves, we know that there will be no violations of any assumptions (these topics will be dealt with in later courses). But still let us have a look into the data:

```{r}
# look into the data by generating scatter plot of x and y
ggplot(dat, aes(x = x, y = y)) + 
    geom_point() + 
    labs(x = "X", y = "Y") + 
    theme_bw()
```
We see from the scatter plot that there is a positive, but not too strong, relationship between `y` and `x` in our data (of course, we've generated the data ourselves). The question is whether we have enough data to reject the null hypothesis that $H_0 : \hat\beta_1 = 0$. To figure this out, use the ordinary least squares estimator via the `lm` function in R.


The first argument of the `lm` function has to be a `formula` that consists of the outcome variable followed by a `~` and, thereafter, the predictor variable. The variable names have to match the names of our data. In our case, this would look like `y ~ x`. The second argument of the `lm` function is the name of the dataset on which we want to fit the model. This would be the `dat` object, we have just created.

```{r}
# fit linear regression
fit = lm(y ~ x, dat)
```

To see a summary of the fitted model, we call the `summary` function on the `fit` object:

```{r}
# print summary of model
summary(fit)
```
- The `Call` line  just reprints how the model was specified. The `Residuals` line shows some summary statistics of the residuals, which are the difference between the actual outcome in our data $y_i$ and the predicted outcome $\hat y_i = \hat \beta_0 + \hat\beta_1 x_i$, i.e., 
$$\hat\epsilon_i = y_i - \hat y_i = y_i - (\hat\beta_0 + \hat\beta_1 x_i).$$

- The table that follows is often called the *regression table*. The (Intercept) row shows statistics for $\hat\beta_0$ (our intercept) and the second row shows statistics for $\hat\beta_1$. In general, the `lm` function will print the variable name of the corresponding coefficient in the first column of the regression table.

- The second column shows our estimates, $(\hat\beta_0, \hat\beta_1)$. Notice that the estimates are not exactly the same as the parameters that we have used to generate the data. This is only natural since there is always random variation involved in sampling. Yet, we can expect that, on average (over infinitely many repeated samples), we will hit the mark. Also, since the OLS estimator is consistent, we can expect that our estimates should become closer and closer to the true parameter values as we increase our sample size. 

>**EXERCISE** Interpret the coefficients. What are they telling us about the relationship between $y_i$ and $x_i$ in our data? What are they telling us about the population?

- The third column shows the estimated standard errors of our estimator. Recall that the estimated standard error is calculated as
$$\widehat{\text{SE}}(\hat\beta_0) = \hat\sigma\sqrt{\frac{ \sum_{i=1}^n x_i^2}{n\sum_{i=1}^n (x_i - \bar x)^2}},$$
and
$$ \widehat{\text{SE}}(\hat\beta_1) = \frac{\hat\sigma}{\sqrt{\sum_{i=1}^n (x_i - \bar x)^2}},$$
where $\hat\sigma$ is the standard error of the regression.

- The fourth column of the regression table shows the test statistics. As the null hypothesis we are testing is that $\beta_k = 0$ for $k = 1,2$, the test statistic we are using is just the the values of the second column divided by the third column:
$$ t_{\beta_k} = \frac{\hat\beta_k}{\widehat{\text{SE}}(\hat\beta_k)}.$$

- The last column of the regression table shows the p-value for each test statistic. 

>**EXERCISE** Based on these results would you reject or not reject the null hypotheses $\beta_0 = 0$ and $\beta_1 = 0$?

- Lastly, the `Residual standard error` is what we have called the standard error of the regression, $\hat \sigma$. It is our estimate for the conditional variance of error term of the model and calculated as
$$\hat\sigma = \sqrt{\frac{\sum_{i=1}^n (y_i - \hat y_i)^2}{n - K}} =  \sqrt{\frac{\sum_{i=1}^n \hat\epsilon_i^2}{n - K}},$$
where $K = 2$ is the number of regression coefficients we have estimated.

To obtain the predicted values of $y_i$, we can use the `predict` function, based on which residuals can be calculated as well:
```{r}
# get predicted values b0 + b1 * x
yhat = predict(fit, type = "response")
# calculated residuals
ehat = y - yhat
```
>**EXERCISE** (Producing the Regression Table by Hand)
>
>1. Calculate the variance of `x`, the mean of `x`, the mean of `y`, and the covariance between `x` and `y`. Using these values, calculate $\hat\beta_0$ and $\hat\beta_1$.
>1. Using `ehat`, calculate $\hat\sigma$. Compare your results with that of the regression table.
>2. Use this estimate to calculate the standard error for $\hat\beta_1$.
>3. Use this estimate, in turn, to calculate the test statistic for $\hat\beta_1$.
>4. Use `yhat` predicted above and the mean of `y` to calculate the total sum of squared (SST) and the regression sum of squared (SSR) and the coefficient of determination. 

---

**Solution** 

```{r eval = F, echo = T}
## sample size and number of variables -----------
n = nrow(dat)
k = ncol(dat)

## b0 and b1 -------------------------------------

# get variance of x, mean of x, and mean of y
var_x = dat[, var(x)]
mean_x = dat[, mean(x)]
mean_y = dat[, mean(y)]
# get covariance
cov_xy = dat[, cov(x,y)]
# calculate beta1
beta1 = cov_xy / var_x
# calculate beta0
beta0 = mean_y - beta1 * mean_x

## test statistic and SE of b1 -------------------

# mse
sigma_hat = sqrt(sum(ehat^2) / (n - k))
# se
se_b1 = sigma_hat / sqrt((n - 1) * var_x)
# test statistic
t1 = beta1 / se_b1

## R-squared --------------------------------------

sst = dat[, sum((y - mean_y)^2)]
ssr = dat[, sum((y - yhat)^2)]
r2 = (sst - ssr) / sst
```

---

We can, in fact, the whole regression table as a `matrix` for further use:

```{r}
# get regression table
regtable = summary(fit)$coefficients
regtable
```
or can use our predicted values to overlay a line to our previous scatter plot:
```{r}
# add predicted values to dataset
dat$yhat = yhat

# plot
ggplot(dat, aes(x = x, y = y)) + 
    geom_point() + 
    geom_line( # add regression line
        aes(y = yhat), # new y-axis
        col = "blue", size = 1.2) + 
    labs(x = "X", y = "Y") + 
    theme_bw()
```
At this point, you should wonder about the following: "is it okay just to add a new column to `dat`? How can we be sure that the `i`th element in `yhat` is indeed the predicted value for the `i`th row of `dat`?" The reason for why this works is because, when fitting the regression, we have specified the dataset to be `dat`. Recall that we used
```r
fit = lm(y ~ x, data = dat)
```
and then used the `fit` object to predict `yhat` by
```r
yhat = predict(fit)
```
As we specified `data = dat` in the first code, the `fit` object will contain information of the `dat` object on which we have fitted the regression. When we then call `predict` on the `fit` object, it will use this information to make sure that the predictions in `yhat` match with the rows in `dat`.

A last topic we might talk about is the uncertainty in our sample regression line, which we have just plotted. As the estimated regression coefficients will be vary across samples, so will our regression line vary as well. And similarly to the way we create confidence intervals for parameters, we might calculate confidence intervals for the regression line. In R, you can do this by using the `predict` function with the `interval = "confidence"` option:
```{r}
# predict confidence interval for the regression line
reg_ci = predict(fit, 
                 interval = "confidence",
                 level = .95)

# check class of object
class(reg_ci) # it's a matrix!

# check names of columns
colnames(reg_ci) 

# add lower and upper bound to dataset
dat$lo = reg_ci[, "lwr"]
dat$hi = reg_ci[, "upr"]
            
# add 95% CI for regression line to the plot
ggplot(dat, aes(x = x, y = y)) + 
    geom_point() + 
    geom_ribbon( # add shaded area for ci
        aes(ymin = lo, ymax = hi), # specify lo and hi
        fill = "blue", 
        col = NA,
        alpha = .4
    ) + 
    geom_line(data = data.table(x = dat$x, y = yhat),
              col = "blue", size = 1.2) + 
    labs(x = "X", y = "Y") + 
    theme_bw()
```
The shaded area is the 95% confidence interval for the regression line. Note that this is **not** the range of values into which we expect *observations* to fall with probability .95 (just look at the plot!). It's the area into which we expect the (sample) regression line to fall 95% of the time if we would sample infinitely many times from the same data-generating process and draw the line $\hat y = \hat\beta_0 + \hat\beta_1 x$. You can think of it as predicting the conditional mean  for all points on the x-axis; for each point $x$, you'll get a confidence interval for the predicted value $\hat y = \hat\beta_0 + \hat\beta_1 x$ and connecting the lower- and upper-bounds of this confidence interval over the range of $X$ will create the blue area shown in the plot.

<br>

# Appendix: Response to Josh's Question (Shape of Confidence Intervals of Regression Lines)

This section is devoted to Josh's question about the shape of the confidence interval of the regression line: why is it narrower in the middle and wider at the end?

**This part is not required for the class and only provided for the extremely curious with some background in linear algebra**. I thought I will leave it here for those of you who have done some linear alebra, or just so that you can come back after you have studied it a little bit. The textbook by Fox that is in the recommendation list would give you sufficient background to understand what follows. But, to reiterate, *it's perfectly fine (and even natural) if you are confused from the very first sentence onward.*

First let us establish some notation. Let $\mathbf{y}_n$ be a $n\times 1$ random vector of outcomes and $\mathbf{X}_n$ be a $n\times k$ matrix of predictors in which the first column is filled with ones. We use lower case letters for observed data as well as random variables.

The OLS estimator of the regression coefficients in a linear regression model is given as

$$ \boldsymbol{\hat\beta} = (\mathbf{X}_n'\mathbf{X}_n)^{-1}\mathbf{X}_n'\mathbf{y}_n.$$

Under the assumption of homoskedasticity, the covariance matrix of $\boldsymbol{\hat\beta}$ is 

$$ \text{Cov}[\boldsymbol{\hat\beta}\,\vert\,\textbf{X}_n] = \sigma^2(\mathbf{X}_n'\mathbf{X})^{-1} =  \frac{\sigma^2}{n} \left(\frac{\mathbf{X}_n'\mathbf{X}_n}{n}\right)^{-1}.$$

Also, by the multivariate Central Limit Theorem, the statistic

$$ \sqrt{n}(\boldsymbol{\hat\beta} - \boldsymbol{\beta})$$

converges to a random vector $\mathbf z$ as $n \rightarrow \infty$, where $\mathbf{z}$ has distribution

$$ \mathbf z \sim \text{Normal}\Big(\mathbf{0}, \sigma^2\mathbf{V}^{-1}\Big)$$

and where

$$\mathbf{V} = \text{plim}_{n\rightarrow\infty}\left(\frac{\mathbf{X}_n'\mathbf{X}_n}{n}\right)$$

is assumed to be a positive-definite matrix (this requires some assumptions regarding the behavior of $\mathbf{X}_n$ as $n$ grows large).

This suggests that we can approximate the distribution of $\boldsymbol{\hat\beta}$ by

$$\boldsymbol{\hat\beta}\sim \text{Normal}\left(\boldsymbol{\beta}, \frac{\hat\sigma^2}{n} \mathbf{\hat V}^{-1}\right)$$

in large samples, where $\mathbf{\hat V} =  \left(\frac{\mathbf{X}_n'\mathbf{X}_n}{n}\right)$. 

Now, our predicted value for the conditional mean of the outcome at $X = x_i$ is given as $\hat y_i = \mathbf{x}_i'\boldsymbol{\hat\beta}$, where $\textbf{x}_i = [1, x_i]'$. But as $\boldsymbol{\hat\beta}$ is Normally distributed, $\hat y_i$, being a linear combination of $\boldsymbol{\hat\beta}$, is Normally distributed as well for large $n$, with mean equal to

$$\text{E}[\hat y_i\,\vert\,\mathbf X_n] = \mathbf{x}_i'\text{E}[\hat\beta\,\vert\,\mathbf{X}_n] = \mathbf{x}_i'\boldsymbol{\beta}$$

and variance

$$\text{Var}[\hat y_i\,\vert\,\textbf{X}_n] = \text{Var}[\mathbf{x}_i'\boldsymbol{\hat\beta}|\mathbf{X}_n]=  \frac{\sigma^2\mathbf{x}_i'\mathbf{V}^{-1}\mathbf{x}_i}{n} = v(\hat y_i).$$

Hence we might estimate the variance of $\hat y_i$ by the consistent estimator

$$\widehat{\text{Var}}[\hat y_i\,\vert\,\textbf{X}_n] = \widehat{\text{Var}}[\mathbf{x}_i'\boldsymbol{\hat\beta}|\mathbf{X}_n]=  n^{-1}\hat\sigma^2\mathbf{x}_i'\mathbf{\hat V}^{-1}\mathbf{x}_i = \hat v(\hat y_i),$$

and construct an approximate 95% confidence interval for $\hat y_i$ as

$$ \hat y_i \pm 1.96 \sqrt{\hat v(\hat y_i)}.$$

What we have called above the 95% confidence interval for the regression line above is in essence a series of point-wise confidence intervals for different $x_i$ values. In other words, for each point $X=x_i$ in our sample, we calculate the confidence interval for $\hat y_i$, after which the lower- and upper-bounds are connected by lines.

We might check this in R:

```{r}
# get coefficients
b  = coef(fit)
# get covariance matrix (sigma_hat / n) * inv(X'X / n)
VC = vcov(fit)

# pointwise se at point x
pwise_int = function(x, level, beta, VC) {

    # get prediction at x
    a = c(1, x)
    p = sum(a * beta)

    # get se of prediction at x
    se = sqrt(t(a) %*% VC %*% a)

    # return lower and upper intervals
    return(c(p - 1.96 * se, p + 1.96 * se))
}

# get CIs
ci_hand = sapply(
    dat$x,
    pwise_int,
    level = .95,
    beta = b,
    VC = VC
)
ci_hand = t(ci_hand)
```
Plotting the two confidence intervals against each other, we see they are the same (up to rounding error):
```{r, fig.width = 10, fig.height = 6}
par(mfrow = c(1, 2))
# compare lower bound
plot(reg_ci[, 2] , ci_hand[, 1],
     axes = T, ann = F,  pch = 19,
     col = "grey")
abline(a = 0, b = 1, col = scales::alpha("purple", .5))
box()
r = round(cor(reg_ci[, 2] , ci_hand[, 1]), 3)
mtext(
    bquote(
        "Lower Bound ("~rho~"="~.(r)~")"
    ),
    side = 3
)

# compare upper bound
plot(reg_ci[, 3] , ci_hand[, 2],
     axes = T, ann = F, pch = 19,
     col = "grey")
abline(a = 0, b = 1, col = scales::alpha("purple", .5))
box()
r = round(cor(reg_ci[, 3] , ci_hand[, 2]), 3)
mtext(
    bquote(
        "Upper Bound ("~rho~"="~.(r)~")"
    ),
    side = 3
)
```

But the width of the confidence interval is a function of standard error, which is in turn a strictly increasing function of the estimated variance $\widehat{\text{Var}}[\hat y_i\,\vert\,\mathbf{X}_n]$. Thus, we can find the point at which the confidence interval is the narrowest by finding the value $x^\ast$ which minimizes the variance.

Let $\mathbf{w} = [1, x]'$, where $x$ is a generic value of that the random variable $X$ may take on. As other terms in the formulation of the variance are fixed, it suffices to consider the function

$$\begin{aligned}
g(x) &= \mathbf{w}'\mathbf{\hat V}^{-1}\mathbf{w} \\
&=\sum_{k=1}^K\sum_{l=1}^K w_k w_lv_{kl}^{-1}.
\end{aligned}$$

to find the value of $x$ at which the CI is minimized, where $v_{kl}^{-1}$ is the $(k,l)$th entry of the matrix $\mathbf{\hat V}^{-1}$. 

We are lucky in this case, since the first element of $\mathbf{w}$ is equal to one, $\mathbf{V}^{-1}$ is symmetric, and $\mathbf{\hat V}$ is only a $2\times 2$ matrix. We have only one moving piece and the inverse of $\mathbf{\hat V}$ can be directly calculated by hand, so we won't need to rely on matrix calculus or more advanced mathematics (although those solutions would be more general). 

First, we use the fact that $w_1 = 1$, and rewrite the objective function as

$$g(x) = v_{11}^{-1} + 2xv_{12}^{-1} + x^2v_{22}^{-1}.$$

As this is a quadratic function, the value $x^\ast$ that minimizes $g$ is 

$$x^\ast = -\frac{v_{12}^{-1}}{v_{22}^{-1}}$$

(Just differentiate with respect to $x$, set the result to zero, and solve for $x$). But the elements of $\mathbf{\hat V}^{-1}$ are 

$$\mathbf{\hat V}^{-1} = \frac{n}{\text{det}(\mathbf{\hat V})} \begin{bmatrix}
\sum_{i=1}^n x_i^2 & - \sum_{i=1}^n x_i \\
- \sum_{i=1}^n x_i & n
\end{bmatrix}$$

Plugging in the right values, noting that the determinant and $n$ are canceled out in the numerator and denominator, we obtain 
$$x^\ast = \frac{1}{n}\sum_{i=1}^n x_i$$

the sample mean. 

As smaller variances lead to narrower CIs and the sample mean minimizes the variance of the predited value, it follows that the CI will be the narrowest at the sample mean. Also, from the functional form of $g$, we see that the variance will grow wider at a quadratic rate as we move away from the sample mean. Since the square-root of a quadratic function is a convex function, it also explains why the CIs become wider in the shape they do in the plot above.

Notice that this stresses an important point which is often misunderstood. It is easy to assume that the width of the CI will be narrower in the areas of the covariate space where we have the most data points. This is, however, not true; it will be narrower at the sample mean of the predictor, *regardless of how many data points we observe at/near the mean*.

An intuitive explantion of why this is so would require another document. The gist of it lies in the fact that we are imposing a very restrictive functional form on the model. Thus, the conclusion would be different if we had discretized the predictor variable into a set of dummy variables. For those of you who have read on until here, I encourage you to run some simulations where $X$ has a bimodal shape with the mean being placed between the modes. Run just a few simulations, maybe 5. In each of the runs, simulate $(X,Y)$, fit a regression, and plot the line against the points. Try to remember the distribution of the points and the line, but save the equation of the regression line as well. Then plot these lines you got from each simulation run in a separate graph. You'll see how the regression line behaves and might get an understanding of why it cannot be otherwise.